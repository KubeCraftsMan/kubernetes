apiVersion: v1
kind: Pod
metadata:
  name: vllm-cpu
  namespace: llamastack
spec:
  containers:
  - name: vllm
    image: vllm/vllm:latest  # ajuste para a versão que você usa
    env:
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            name: hf-token
            key: HF_TOKEN  # o nome da chave que você colocou no secret
    command: ["python", "-m", "vllm.entrypoints.openai.api_server"]
    args:
      - "--model"
      - "meta-llama/Llama-3.2-1B-Instruct"
      - "--dtype"
      - "bfloat16"
      - "--max-num-batched-tokens"
      - "131072"          # igual ao max_model_len, evitando erros de SchedulerConfig
      - "--num-cores"
      - "4"               # ajuste de acordo com os núcleos do pod
      - "--disable-gpu"    # força CPU-only
    resources:
      limits:
        cpu: "4"          # ajuste conforme a disponibilidade
        memory: "8Gi"     # ajuste conforme a disponibilidade
      requests:
        cpu: "2"
        memory: "4Gi"
