apiVersion: apps/v1
kind: Deployment
metadata:
  name: llamastack
  namespace: llamastack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llamastack
  template:
    metadata:
      labels:
        app: llamastack
    spec:
      containers:
        # Sidecar: Ollama
        - name: ollama
          image: ollama/ollama:latest
          ports:
            - containerPort: 11434
          resources:
            requests:
              cpu: "500m"         # garante meio core reservado
              memory: "1Gi"       # garante 1Gi mínimo
            limits:
              cpu: "1"            # não passa de 1 core
              memory: "2Gi"       # não passa de 2Gi

        # Container principal: Llama Stack
        - name: llamastack
          image: llamastack/distribution-ollama:latest
          ports:
            - containerPort: 5001
          env:
            - name: INFERENCE_MODEL
              value: "llama3.2:3b"
            - name: OLLAMA_URL
              value: "http://localhost:11434"
          resources:
            requests:
              cpu: "500m"
              memory: "1Gi"
            limits:
              cpu: "1"
              memory: "2Gi"
